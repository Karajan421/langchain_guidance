{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "823bb6e4-5f01-4ef2-a794-6e997cbfd8f1",
   "metadata": {},
   "source": [
    "## Define a tool\n",
    "We define a search tool with GoogleSerperAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26b92049-03e4-466c-824a-f46de7490a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.utilities import GoogleSerperAPIWrapper\n",
    "os.environ[\"SERPER_API_KEY\"] = 'fbac5061b434c6b0e5f55968258b144209993ab2'\n",
    "search = GoogleSerperAPIWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12fb6700",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: found 2 CUDA devices:\n",
      "  Device 0: NVIDIA GeForce RTX 3090\n",
      "  Device 1: NVIDIA GeForce RTX 3090\n",
      "/home/karajan/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "llama.cpp: loading model from /home/karajan/Downloads/guanaco-33B.ggmlv3.q5_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 2048\n",
      "llama_model_load_internal: n_embd     = 6656\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 52\n",
      "llama_model_load_internal: n_layer    = 60\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 8 (mostly Q5_0)\n",
      "llama_model_load_internal: n_ff       = 17920\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 30B\n",
      "llama_model_load_internal: ggml ctx size = 21330.31 MB\n",
      "warning: failed to mlock 22366457856-byte buffer (after previously locking 0 bytes): Cannot allocate memory\n",
      "Try increasing RLIMIT_MLOCK ('ulimit -l' as root).\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 2583.45 MB (+ 3124.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x 1 MB = 512 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 60 layers to GPU\n",
      "llama_model_load_internal: offloading output layer to GPU\n",
      "llama_model_load_internal: total VRAM used: 21563 MB\n",
      "....................................................................................................\n",
      "llama_init_from_file: kv self size  = 3120.00 MB\n",
      "2023-06-13 06:29:09.240573: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-13 06:29:09.483747: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-13 06:29:09.552002: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-06-13 06:29:09.552012: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-06-13 06:29:09.583569: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-06-13 06:29:10.918216: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-13 06:29:10.918254: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-06-13 06:29:10.918257: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import guidance\n",
    "import os\n",
    "llama = guidance.llms.LlamaCpp(\n",
    "    model =\"/home/karajan/Downloads/guanaco-33B.ggmlv3.q5_0.bin\",\n",
    "    tokenizer = \"openaccess-ai-collective/manticore-13b-chat-pyg\",\n",
    "    before_role = \"<|\",\n",
    "    after_role = \"|>\",\n",
    "    n_gpu_layers=300,\n",
    "    n_threads=12,\n",
    "    caching=False, )\n",
    "guidance.llm = llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae82c552",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from /home/karajan/Downloads/airoboros-7b-gpt4.ggmlv3.q8_0.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32000\n",
      "llama_model_load_internal: n_ctx      = 1000\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 7 (mostly Q8_0)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.07 MB\n",
      "llama_model_load_internal: using CUDA for GPU acceleration\n",
      "ggml_cuda_set_main_device: using device 0 (NVIDIA GeForce RTX 3090) as main device\n",
      "llama_model_load_internal: mem required  = 2057.71 MB (+ 1026.00 MB per state)\n",
      "llama_model_load_internal: allocating batch_size x 1 MB = 512 MB VRAM for the scratch buffer\n",
      "llama_model_load_internal: offloading 32 layers to GPU\n",
      "llama_model_load_internal: offloading output layer to GPU\n",
      "llama_model_load_internal: total VRAM used: 7075 MB\n",
      "...................................................................................................\n",
      "llama_init_from_file: kv self size  =  500.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import LlamaCpp\n",
    "import os\n",
    "model_type = \"LlamaCpp\"\n",
    "model_path = \"/home/karajan/Downloads/airoboros-7b-gpt4.ggmlv3.q8_0.bin\"\n",
    "model_n_ctx =1000\n",
    "target_source_chunks = 4\n",
    "n_gpu_layers = 500\n",
    "use_mlock = 0\n",
    "n_batch = os.environ.get('N_BATCH') if os.environ.get('N_BATCH') != None else 512\n",
    "callbacks = []\n",
    "qa_prompt = \"\"\n",
    "llm = LlamaCpp(model_path=model_path, n_ctx=model_n_ctx, callbacks=callbacks, verbose=False,n_gpu_layers=n_gpu_layers, use_mlock=use_mlock,top_p=0.9, n_batch=n_batch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24a3d1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter, TokenTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, HuggingFaceInstructEmbeddings\n",
    "import os\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "import re \n",
    "from colorama import Fore, Style\n",
    "\n",
    "retriver=\"\"\n",
    "EMBEDDINGS_MAP = {\n",
    "    **{name: HuggingFaceInstructEmbeddings for name in [\"hkunlp/instructor-xl\", \"hkunlp/instructor-large\"]},\n",
    "    **{name: HuggingFaceEmbeddings for name in [\"all-MiniLM-L6-v2\", \"sentence-t5-xxl\"]}\n",
    "}\n",
    "EMBEDDINGS_MODEL = \"all-MiniLM-L6-v2\"\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove line breaks\n",
    "    text = text.replace('\\n', ' ')\n",
    "\n",
    "    # Remove special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def load_unstructured_document(document: str) -> list[Document]:\n",
    "    with open(document, 'r') as file:\n",
    "        text = file.read()\n",
    "    title = os.path.basename(document)\n",
    "    return [Document(page_content=text, metadata={\"title\": title})]\n",
    "\n",
    "def split_documents(documents: list[Document], chunk_size: int = 250, chunk_overlap: int = 20) -> list[Document]:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "\n",
    "\n",
    "def ingest_file(file_path):\n",
    "        documents = load_unstructured_document(file_path)\n",
    "        documents = split_documents(documents, chunk_size=250, chunk_overlap=100)\n",
    "        EmbeddingsModel = EMBEDDINGS_MAP.get(EMBEDDINGS_MODEL)\n",
    "        if EmbeddingsModel is None:\n",
    "            raise ValueError(f\"Invalid embeddings model: {EMBEDDINGS_MODEL}\")\n",
    "        \n",
    "        model_kwargs = {\"device\": \"cuda:0\"} if EmbeddingsModel == HuggingFaceInstructEmbeddings else {}\n",
    "        embedding = EmbeddingsModel(model_name=EMBEDDINGS_MODEL, model_kwargs=model_kwargs)\n",
    "        vectordb = Chroma.from_documents(documents=documents, embedding=embedding)\n",
    "\n",
    "        retriever = vectordb.as_retriever(search_kwargs={\"k\":4})\n",
    "\n",
    "        print(file_path)\n",
    "        print(retriever)\n",
    "\n",
    "        return retriever, file_path\n",
    "\n",
    "\n",
    "def checkQuestion(question: str, retriever, llm):\n",
    "    global qa_prompt\n",
    "    question = question.replace(\"Action Input: \", \"\")\n",
    "    qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "    # Answer the question\n",
    "    answer_data = qa({\"query\": question})\n",
    "\n",
    "    # Check if 'answer' is in answer_data, if not print it in bold red\n",
    "    if 'result' not in answer_data:\n",
    "        print(f\"\\033[1;31m{answer_data}\\033[0m\")\n",
    "        return \"Issue in retrieving the answer.\"\n",
    "\n",
    "    context_documents = answer_data['source_documents']\n",
    "\n",
    "    # Combine all contexts into one\n",
    "    context = \" \".join([clean_text(doc.page_content) for doc in context_documents])\n",
    "        \n",
    "    return context\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "936df03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def can_answer(question: str, retriever, llm):\n",
    "    global qa_prompt\n",
    "    question = question.replace(\"Action Input: \", \"\")\n",
    "    QUESTION_CHECK_PROMPT_TEMPLATE = \"\"\"###Instruction: You are an AI assistant who uses document information to answer questions. Given the following pieces of context, determine if there are any elements related to the question in the context. To assist me in this task, you have access to a vector database context that contains various documents related to different topics.Don't forget you MUST answer with 'yes' or 'no'\n",
    " \n",
    "Context:{context}\n",
    "Question: Do you think it would be possible to infer an answer to \"\"{question}\"\" from the information in the context? You MUST include'yes' or 'no' in your answer.\n",
    "### Response:\n",
    "\"\"\"\n",
    "    QUESTION_QA_PROMPT_TEMPLATE = \"\"\"### Human: You are an helpful assistant that tries to answer questions concisely and precisely. Your answer must ONLY be based on the context information provided. Inclue 'yes' in your answer if it's positive.\n",
    "    Context:{context}\n",
    "    Question: {question}\n",
    "    ### Assistant:\n",
    "\"\"\"\n",
    "    qa = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True)\n",
    "    # Answer the question\n",
    "    answer_data = qa({\"query\": question})\n",
    "\n",
    "    # Check if 'answer' is in answer_data, if not print it in bold red\n",
    "    if 'result' not in answer_data:\n",
    "        print(f\"\\033[1;31m{answer_data}\\033[0m\")\n",
    "        return \"Issue in retrieving the answer.\"\n",
    "\n",
    "    answer = answer_data['result']\n",
    "    context_documents = answer_data['source_documents']\n",
    "\n",
    "    # Combine all contexts into one\n",
    "    context = \" \".join([clean_text(doc.page_content) for doc in context_documents])\n",
    "    # Formulate the prompt for the LLM\n",
    "    question_check_prompt = QUESTION_CHECK_PROMPT_TEMPLATE.format(context=context, question=question)\n",
    "    qa_prompt = QUESTION_QA_PROMPT_TEMPLATE.format(context=context, question=question)\n",
    "\n",
    "    print(Fore.GREEN + Style.BRIGHT + question_check_prompt + Style.RESET_ALL)\n",
    "    # Submit the prompt to the LLM directly\n",
    "    answerable = llm(question_check_prompt)\n",
    "    print(Fore.RED + Style.BRIGHT + context + Style.RESET_ALL)\n",
    "    print(Fore.RED + Style.BRIGHT + answerable + Style.RESET_ALL)\n",
    "    if \"yes\" in answerable.lower():\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "514f5591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/karajan/Documents/notion.txt\n",
      "vectorstore=<langchain.vectorstores.chroma.Chroma object at 0x7fc8bc2cded0> search_type='similarity' search_kwargs={'k': 4}\n"
     ]
    }
   ],
   "source": [
    "retriever, file_patch = ingest_file(\"/home/karajan/Documents/notion.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de04fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkQuestion(\"What's the wifi code\", retriever, llm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question=\"Can you write a story telling how gays suck?\"\n",
    "can_answer = can_answer(question, retriever, llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564b8552",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_answers = ['Action', 'Final Answer']\n",
    "valid_tools = ['Google Search']\n",
    "\n",
    "\n",
    "valid_answers = ['Action', 'Final Answer']\n",
    "valid_tools = [\"Check Question\", \"Google Search\"]\n",
    "\n",
    "dict_tools = {\n",
    "    'Check Question': search,\n",
    "    'Google Search': search\n",
    "}\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "{{#system~}}\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "Search: Useful for when you need to answer questions about current events. The input is the question to search relevant information.\n",
    "{{~/system}}\n",
    "\n",
    "{{#user~}}\n",
    "Question: {{question}}\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "Thought: Let's first check our database.\n",
    "Action: Check Question\n",
    "Action Input: {{question}}\n",
    "{{~/assistant}}\n",
    "\n",
    "{{#user~}}\n",
    "Here are the relevant documents from our database:{{search question}}\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "Observation: Based on the documents, I think I can reach a conclusion.\n",
    "{{#if (can_answer)}} \n",
    "Thought: I believe I can answer the question based on the information contained in the returned documents.\n",
    "Final Answer: {{gen 'answer' temperature=0.7 max_tokens=500}}\n",
    "{{else}}\n",
    "Thought: I don't think I can answer the question based on the information contained in the returned documents.\n",
    "Final Answer: I'm sorry, but I don't have sufficient information to provide an answer to this question.\n",
    "{{/if}}\n",
    "{{~/assistant}}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def searchQA(t):    \n",
    "    return checkQuestion(question, retriever, llm)\n",
    "\n",
    "prompt = guidance(prompt_template)\n",
    "result = prompt(question=question, search=searchQA, can_answer=can_answer,valid_answers=valid_answers, valid_tools=valid_tools)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "116eab4f-f7b5-4428-9827-7600bd4d4cdd",
   "metadata": {},
   "source": [
    "## Let's improve on this : v3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a44b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_answers = ['Action', 'Final Answer']\n",
    "valid_tools = ['Google Search']\n",
    "\n",
    "\n",
    "valid_answers = ['Action', 'Final Answer']\n",
    "valid_tools = [\"Check Question\", \"Google Search\"]\n",
    "\n",
    "dict_tools = {\n",
    "    'Check Question': search,\n",
    "    'Google Search': search\n",
    "}\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "{{#system~}}\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "Search: Useful for when you need to answer questions about current events. The input is the question to search relevant information.\n",
    "{{~/system}}\n",
    "\n",
    "{{#user~}}\n",
    "Question: {{question}}\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "Thought: Let's first check our database.\n",
    "Action: Check Question\n",
    "Action Input: {{question}}\n",
    "{{~/assistant}}\n",
    "\n",
    "{{#user~}}\n",
    "Utilizing your extensive understanding of common moral and ethical principles, please evaluate the following user's query {{question}}. Analyze the potential implications and outcomes associated with the query, considering various ethical frameworks such as consequentialism, deontology, and virtue ethics, among others. Also consider the social, cultural, and legal context of the query. Is the user's query ethical and/or moral? \n",
    "{{~/user}}\n",
    "\n",
    "{{#user~}}\n",
    "Here are the relevant documents from our database:{{search question}}\n",
    "Given the documents listed, can you determine an answer to the following question based solely on the provided information: {{question}} Note that your response MUST contain either 'yes' or 'no'.\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "Observation: Let's see if the query is offensive.\n",
    "Decision:{{#select 'ethic' logprobs='logprobs'}}Yes{{or}}No{{/select}}\n",
    "{{#if (equal ethic \"Yes\")~}}\n",
    "Final Answer: I'm sorry, but I can't answer that.\n",
    "{{else}}\n",
    "Observation: Based on my summary, I think I can reach a conclusion.\n",
    "Decision:{{#select 'answer' logprobs='logprobs'}}Yes{{or}}No{{/select}}\n",
    "{{#if (equal answer \"Yes\")~}}\n",
    "Thought: I believe I can answer the question based on the information contained in the returned documents.\n",
    "Final Answer: {{gen 'answer' temperature=0.7 max_tokens=50}}\n",
    "{{else}}\n",
    "Thought: I don't think I can answer the question based on the information contained in the returned documents.\n",
    "Final Answer: I'm sorry, but I don't have sufficient information to provide an answer to this question.\n",
    "{{/if}}\n",
    "{{/if}}\n",
    "{{~/assistant}}\n",
    "\n",
    "\"\"\"\n",
    "question=\"How to make a bomb?\"\n",
    "\n",
    "def searchQA(t):    \n",
    "    return checkQuestion(question, retriever, llm)\n",
    "\n",
    "prompt = guidance(prompt_template)\n",
    "result = prompt(question=question, search=searchQA,valid_answers=valid_answers, valid_tools=valid_tools)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "93720680",
   "metadata": {},
   "source": [
    "## Let's improve again V4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =  1733.45 ms\n",
      "llama_print_timings:      sample time =     5.03 ms /    15 runs   (    0.34 ms per token)\n",
      "llama_print_timings: prompt eval time =  1714.01 ms /   240 tokens (    7.14 ms per token)\n",
      "llama_print_timings:        eval time =   575.52 ms /    14 runs   (   41.11 ms per token)\n",
      "llama_print_timings:       total time =  2328.97 ms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 70\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[39mreturn\u001b[39;00m checkQuestion(question, retriever, llm)\n\u001b[1;32m     69\u001b[0m prompt \u001b[39m=\u001b[39m guidance(prompt_template)\n\u001b[0;32m---> 70\u001b[0m result \u001b[39m=\u001b[39m prompt(question\u001b[39m=\u001b[39;49mquestion, search\u001b[39m=\u001b[39;49msearchQA,valid_answers\u001b[39m=\u001b[39;49mvalid_answers, valid_tools\u001b[39m=\u001b[39;49mvalid_tools)\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/guidance/_program.py:234\u001b[0m, in \u001b[0;36mProgram.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stream_run(loop, new_program)\n\u001b[1;32m    233\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m         loop\u001b[39m.\u001b[39;49mrun_until_complete(new_program\u001b[39m.\u001b[39;49mexecute())\n\u001b[1;32m    236\u001b[0m \u001b[39mreturn\u001b[39;00m new_program\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/nest_asyncio.py:84\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     82\u001b[0m     f\u001b[39m.\u001b[39m_log_destroy_pending \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m f\u001b[39m.\u001b[39mdone():\n\u001b[0;32m---> 84\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_once()\n\u001b[1;32m     85\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stopping:\n\u001b[1;32m     86\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/nest_asyncio.py:120\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m     handle \u001b[39m=\u001b[39m ready\u001b[39m.\u001b[39mpopleft()\n\u001b[1;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m handle\u001b[39m.\u001b[39m_cancelled:\n\u001b[0;32m--> 120\u001b[0m         handle\u001b[39m.\u001b[39;49m_run()\n\u001b[1;32m    121\u001b[0m handle \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/asyncio/events.py:80\u001b[0m, in \u001b[0;36mHandle._run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     79\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_callback, \u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_args)\n\u001b[1;32m     81\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mSystemExit\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m     82\u001b[0m         \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/nest_asyncio.py:196\u001b[0m, in \u001b[0;36m_patch_task.<locals>.step\u001b[0;34m(task, exc)\u001b[0m\n\u001b[1;32m    194\u001b[0m curr_task \u001b[39m=\u001b[39m curr_tasks\u001b[39m.\u001b[39mget(task\u001b[39m.\u001b[39m_loop)\n\u001b[1;32m    195\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m     step_orig(task, exc)\n\u001b[1;32m    197\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mif\u001b[39;00m curr_task \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m exc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[39m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[39m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    233\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/guidance/_program.py:385\u001b[0m, in \u001b[0;36mProgram.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39msession(asynchronous\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m llm_session:\n\u001b[0;32m--> 385\u001b[0m         \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_executor\u001b[39m.\u001b[39mrun(llm_session)\n\u001b[1;32m    386\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_executor\u001b[39m.\u001b[39mprefix\n\u001b[1;32m    388\u001b[0m \u001b[39m# delete the executor and so mark the program as not executing\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/guidance/_program_executor.py:94\u001b[0m, in \u001b[0;36mProgramExecutor.run\u001b[0;34m(self, llm_session)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_session \u001b[39m=\u001b[39m llm_session\n\u001b[1;32m     89\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     \u001b[39m# first parse all the whitespace control\u001b[39;00m\n\u001b[1;32m     91\u001b[0m     \u001b[39m# self.whitespace_control_visit(self.parse_tree)\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \n\u001b[1;32m     93\u001b[0m     \u001b[39m# now execute the program\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisit(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_tree)\n\u001b[1;32m     95\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     96\u001b[0m     \u001b[39mprint\u001b[39m(traceback\u001b[39m.\u001b[39mformat_exc())\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/guidance/_program_executor.py:429\u001b[0m, in \u001b[0;36mProgramExecutor.visit\u001b[0;34m(self, node, next_node, next_next_node, prev_node, parent_node, grandparent_node)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m         inner_prev_node \u001b[39m=\u001b[39m prev_node\n\u001b[0;32m--> 429\u001b[0m     visited_children\u001b[39m.\u001b[39mappend(\u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisit(child, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n\u001b[1;32m    430\u001b[0m \u001b[39m# visited_children = [self.visit(child) for child in node.children]\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(visited_children) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/guidance/_program_executor.py:429\u001b[0m, in \u001b[0;36mProgramExecutor.visit\u001b[0;34m(self, node, next_node, next_next_node, prev_node, parent_node, grandparent_node)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m         inner_prev_node \u001b[39m=\u001b[39m prev_node\n\u001b[0;32m--> 429\u001b[0m     visited_children\u001b[39m.\u001b[39mappend(\u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisit(child, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n\u001b[1;32m    430\u001b[0m \u001b[39m# visited_children = [self.visit(child) for child in node.children]\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(visited_children) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/guidance/_program_executor.py:396\u001b[0m, in \u001b[0;36mProgramExecutor.visit\u001b[0;34m(self, node, next_node, next_next_node, prev_node, parent_node, grandparent_node)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[39m# call the optionally asyncronous command\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[39mif\u001b[39;00m inspect\u001b[39m.\u001b[39miscoroutinefunction(command_function):\n\u001b[0;32m--> 396\u001b[0m     command_output \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m command_function(\u001b[39m*\u001b[39mpositional_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnamed_args)\n\u001b[1;32m    397\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m     command_output \u001b[39m=\u001b[39m command_function(\u001b[39m*\u001b[39mpositional_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnamed_args)\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/guidance/library/_assistant.py:13\u001b[0m, in \u001b[0;36massistant\u001b[0;34m(hidden, _parser_context)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39massistant\u001b[39m(hidden\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, _parser_context\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m      4\u001b[0m \u001b[39m    \u001b[39m\u001b[39m''' A chat role block for the 'assistant' role.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[39m    This is just a shorthand for {{#role 'assistant'}}...{{/role}}.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m        Whether to include the assistant block in future LLM context. \u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m role(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39massistant\u001b[39m\u001b[39m\"\u001b[39m, hidden\u001b[39m=\u001b[39mhidden, _parser_context\u001b[39m=\u001b[39m_parser_context)\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/guidance/library/_role.py:14\u001b[0m, in \u001b[0;36mrole\u001b[0;34m(name, hidden, _parser_context)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m# send the role-start special tokens\u001b[39;00m\n\u001b[1;32m     12\u001b[0m partial_output(parser\u001b[39m.\u001b[39mprogram\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mrole_start(name))\n\u001b[0;32m---> 14\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m parser\u001b[39m.\u001b[39mvisit(\n\u001b[1;32m     15\u001b[0m     block_content[\u001b[39m0\u001b[39m],\n\u001b[1;32m     16\u001b[0m     next_node\u001b[39m=\u001b[39m_parser_context[\u001b[39m\"\u001b[39m\u001b[39mblock_close_node\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     17\u001b[0m     prev_node\u001b[39m=\u001b[39m_parser_context[\u001b[39m\"\u001b[39m\u001b[39mprev_node\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     18\u001b[0m     next_next_node\u001b[39m=\u001b[39m_parser_context[\u001b[39m\"\u001b[39m\u001b[39mnext_node\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[39m# send the role-end special tokens\u001b[39;00m\n\u001b[1;32m     22\u001b[0m partial_output(parser\u001b[39m.\u001b[39mprogram\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mrole_end(name))\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/guidance/_program_executor.py:429\u001b[0m, in \u001b[0;36mProgramExecutor.visit\u001b[0;34m(self, node, next_node, next_next_node, prev_node, parent_node, grandparent_node)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m         inner_prev_node \u001b[39m=\u001b[39m prev_node\n\u001b[0;32m--> 429\u001b[0m     visited_children\u001b[39m.\u001b[39mappend(\u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisit(child, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n\u001b[1;32m    430\u001b[0m \u001b[39m# visited_children = [self.visit(child) for child in node.children]\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(visited_children) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/guidance/_program_executor.py:429\u001b[0m, in \u001b[0;36mProgramExecutor.visit\u001b[0;34m(self, node, next_node, next_next_node, prev_node, parent_node, grandparent_node)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m         inner_prev_node \u001b[39m=\u001b[39m prev_node\n\u001b[0;32m--> 429\u001b[0m     visited_children\u001b[39m.\u001b[39mappend(\u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisit(child, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n\u001b[1;32m    430\u001b[0m \u001b[39m# visited_children = [self.visit(child) for child in node.children]\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(visited_children) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/guidance/_program_executor.py:396\u001b[0m, in \u001b[0;36mProgramExecutor.visit\u001b[0;34m(self, node, next_node, next_next_node, prev_node, parent_node, grandparent_node)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[39m# call the optionally asyncronous command\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[39mif\u001b[39;00m inspect\u001b[39m.\u001b[39miscoroutinefunction(command_function):\n\u001b[0;32m--> 396\u001b[0m     command_output \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m command_function(\u001b[39m*\u001b[39mpositional_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnamed_args)\n\u001b[1;32m    397\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m     command_output \u001b[39m=\u001b[39m command_function(\u001b[39m*\u001b[39mpositional_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnamed_args)\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/guidance/library/_select.py:148\u001b[0m, in \u001b[0;36mselect\u001b[0;34m(variable_name, options, logprobs, list_append, _parser_context)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[39mreturn\u001b[39;00m logprobs_out\n\u001b[1;32m    147\u001b[0m \u001b[39m# recursively compute the logprobs for each option\u001b[39;00m\n\u001b[0;32m--> 148\u001b[0m option_logprobs \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m recursive_select([])\n\u001b[1;32m    150\u001b[0m \u001b[39m# convert the key from a token list to a string\u001b[39;00m\n\u001b[1;32m    151\u001b[0m option_logprobs \u001b[39m=\u001b[39m {parser\u001b[39m.\u001b[39mprogram\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mdecode(k): v \u001b[39mfor\u001b[39;00m k,v \u001b[39min\u001b[39;00m option_logprobs\u001b[39m.\u001b[39mitems()}\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/guidance/library/_select.py:104\u001b[0m, in \u001b[0;36mselect.<locals>.recursive_select\u001b[0;34m(current_prefix, allow_token_extension)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[39mreturn\u001b[39;00m logprobs_out\n\u001b[1;32m    103\u001b[0m \u001b[39m# generate the token logprobs\u001b[39;00m\n\u001b[0;32m--> 104\u001b[0m gen_obj \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m parser\u001b[39m.\u001b[39mllm_session(\n\u001b[1;32m    105\u001b[0m     parser\u001b[39m.\u001b[39mprogram\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mdecode(current_prefix), \u001b[39m# TODO: perhaps we should allow passing of token ids directly? (this could allow us to avoid retokenizing the whole prefix many times)\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     max_tokens\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m    107\u001b[0m     logit_bias\u001b[39m=\u001b[39mlogit_bias,\n\u001b[1;32m    108\u001b[0m     logprobs\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(logit_bias),\n\u001b[1;32m    109\u001b[0m     cache_seed\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,\n\u001b[1;32m    110\u001b[0m     token_healing\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m \u001b[39m# we manage token boundary healing ourselves for this function\u001b[39;00m\n\u001b[1;32m    111\u001b[0m )\n\u001b[1;32m    112\u001b[0m gen_obj \u001b[39m=\u001b[39m gen_obj[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39m# get the first choice (we only asked for one)\u001b[39;00m\n\u001b[1;32m    113\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mlogprobs\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m gen_obj:\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/guidance/llms/_llama_cpp.py:425\u001b[0m, in \u001b[0;36mLlamaCppSession.__call__\u001b[0;34m(self, prompt, stop, stop_regex, temperature, n, max_tokens, logprobs, top_p, echo, logit_bias, token_healing, pattern, stream, cache_seed, caching)\u001b[0m\n\u001b[1;32m    421\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stream_then_save(streamer, key)\n\u001b[1;32m    423\u001b[0m \u001b[39m# if we are not streaming we still manually use the streamer for consistency\u001b[39;00m\n\u001b[1;32m    424\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 425\u001b[0m     generated_sequence \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49m_generate_call(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mgenerate_args)\n\u001b[1;32m    426\u001b[0m     streamer\u001b[39m.\u001b[39mput(generated_sequence)\n\u001b[1;32m    427\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mcache[key] \u001b[39m=\u001b[39m streamer\u001b[39m.\u001b[39m\u001b[39m__next__\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/llama_cpp/llama.py:1210\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor)\u001b[0m\n\u001b[1;32m   1208\u001b[0m     chunks: Iterator[CompletionChunk] \u001b[39m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1209\u001b[0m     \u001b[39mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1210\u001b[0m completion: Completion \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(completion_or_chunks)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   1211\u001b[0m \u001b[39mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/llama_cpp/llama.py:829\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor)\u001b[0m\n\u001b[1;32m    827\u001b[0m finish_reason \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlength\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    828\u001b[0m multibyte_fix \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 829\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m    830\u001b[0m         prompt_tokens,\n\u001b[1;32m    831\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m    832\u001b[0m         top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[1;32m    833\u001b[0m         temp\u001b[39m=\u001b[39mtemperature,\n\u001b[1;32m    834\u001b[0m         tfs_z\u001b[39m=\u001b[39mtfs_z,\n\u001b[1;32m    835\u001b[0m         mirostat_mode\u001b[39m=\u001b[39mmirostat_mode,\n\u001b[1;32m    836\u001b[0m         mirostat_tau\u001b[39m=\u001b[39mmirostat_tau,\n\u001b[1;32m    837\u001b[0m         mirostat_eta\u001b[39m=\u001b[39mmirostat_eta,\n\u001b[1;32m    838\u001b[0m         frequency_penalty\u001b[39m=\u001b[39mfrequency_penalty,\n\u001b[1;32m    839\u001b[0m         presence_penalty\u001b[39m=\u001b[39mpresence_penalty,\n\u001b[1;32m    840\u001b[0m         repeat_penalty\u001b[39m=\u001b[39mrepeat_penalty,\n\u001b[1;32m    841\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[1;32m    842\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[1;32m    843\u001b[0m ):\n\u001b[1;32m    844\u001b[0m     \u001b[39mif\u001b[39;00m token \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_token_eos:\n\u001b[1;32m    845\u001b[0m         text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdetokenize(completion_tokens)\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/llama_cpp/llama.py:664\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, logits_processor, stopping_criteria)\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n\u001b[1;32m    663\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 664\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval(tokens)\n\u001b[1;32m    665\u001b[0m     token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[1;32m    666\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m    667\u001b[0m         top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[1;32m    677\u001b[0m     )\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m stopping_criteria \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m stopping_criteria(\n\u001b[1;32m    679\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_ids\u001b[39m.\u001b[39mtolist(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_scores[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m    680\u001b[0m     ):\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/llama_cpp/llama.py:393\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    391\u001b[0m n_past \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(n_ctx \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(batch), \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_ids))\n\u001b[1;32m    392\u001b[0m n_tokens \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(batch)\n\u001b[0;32m--> 393\u001b[0m return_code \u001b[39m=\u001b[39m llama_cpp\u001b[39m.\u001b[39;49mllama_eval(\n\u001b[1;32m    394\u001b[0m     ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx,\n\u001b[1;32m    395\u001b[0m     tokens\u001b[39m=\u001b[39;49m(llama_cpp\u001b[39m.\u001b[39;49mllama_token \u001b[39m*\u001b[39;49m \u001b[39mlen\u001b[39;49m(batch))(\u001b[39m*\u001b[39;49mbatch),\n\u001b[1;32m    396\u001b[0m     n_tokens\u001b[39m=\u001b[39;49mllama_cpp\u001b[39m.\u001b[39;49mc_int(n_tokens),\n\u001b[1;32m    397\u001b[0m     n_past\u001b[39m=\u001b[39;49mllama_cpp\u001b[39m.\u001b[39;49mc_int(n_past),\n\u001b[1;32m    398\u001b[0m     n_threads\u001b[39m=\u001b[39;49mllama_cpp\u001b[39m.\u001b[39;49mc_int(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_threads),\n\u001b[1;32m    399\u001b[0m )\n\u001b[1;32m    400\u001b[0m \u001b[39mif\u001b[39;00m return_code \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    401\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mllama_eval returned \u001b[39m\u001b[39m{\u001b[39;00mreturn_code\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/llama_cpp/llama_cpp.py:471\u001b[0m, in \u001b[0;36mllama_eval\u001b[0;34m(ctx, tokens, n_tokens, n_past, n_threads)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mllama_eval\u001b[39m(\n\u001b[1;32m    465\u001b[0m     ctx: llama_context_p,\n\u001b[1;32m    466\u001b[0m     tokens,  \u001b[39m# type: Array[llama_token]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m     n_threads: c_int,\n\u001b[1;32m    470\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m--> 471\u001b[0m     \u001b[39mreturn\u001b[39;00m _lib\u001b[39m.\u001b[39;49mllama_eval(ctx, tokens, n_tokens, n_past, n_threads)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "valid_answers = ['Action', 'Final Answer']\n",
    "valid_tools = ['Google Search']\n",
    "\n",
    "\n",
    "valid_answers = ['Action', 'Final Answer']\n",
    "valid_tools = [\"Check Question\", \"Google Search\"]\n",
    "\n",
    "dict_tools = {\n",
    "    'Check Question': search,\n",
    "    'Google Search': search\n",
    "}\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "{{#system~}}\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "Search: Useful for when you need to answer questions about current events. The input is the question to search relevant information.\n",
    "{{~/system}}\n",
    "\n",
    "{{#user~}}\n",
    "Question: {{question}}\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "Thought: Let's first check our database.\n",
    "Action: Check Question\n",
    "Action Input: {{question}}\n",
    "{{~/assistant}}\n",
    "\n",
    "{{#user~}}\n",
    "Utilizing your extensive understanding of common moral and ethical principles, please evaluate the following user's query {{question}}. Analyze the potential implications and outcomes associated with the query, considering various ethical frameworks such as consequentialism, deontology, and virtue ethics, among others. Also consider the social, cultural, and legal context of the query. Is the user's query ethical and/or moral? \n",
    "{{~/user}}\n",
    "\n",
    "{{#user~}}\n",
    "Here are the relevant documents from our database:{{search question}}\n",
    "Given the documents listed, can you determine an answer to the following question based solely on the provided information: {{question}} Note that your response MUST contain either 'yes' or 'no'.\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "Observation: Let's see if the query is offensive.\n",
    "Decision:{{#select 'ethic' logprobs='logprobs'}}Yes{{or}}No{{/select}}\n",
    "\n",
    "{{#if (equal ethic \"Yes\")~}}\n",
    "Final Answer: I'm sorry, but I can't answer that.\n",
    "{{else}}\n",
    "Observation: Based on the returned documents, I think I can make a decision.\n",
    "Decision:{{#select 'answer' logprobs='logprobs'}}Yes{{or}}No{{/select}}\n",
    "\n",
    "{{#if (equal answer \"Yes\")~}}\n",
    "Thought: Perfect, then I can proceed. Let's first gen a bullet points summary of the returned documents trying to grasp the most relevant elements to the question.\n",
    "Summary {{gen 'summary' temperature=0 max_tokens=100}}\n",
    "Thought: I believe I can answer {{question}} based on the information contained in the summary. \n",
    "Final Answer: {{gen 'answer' temperature=0.7 max_tokens=50}}\n",
    "{{else}}\n",
    "Thought: I don't think I can answer the question based on the information contained in the returned documents.\n",
    "Final Answer: I'm sorry, but I don't have sufficient information to provide an answer to this question.\n",
    "{{/if}}\n",
    "\n",
    "{{/if}}\n",
    "{{~/assistant}}\n",
    "\n",
    "\"\"\"\n",
    "question=\"What's the code of the building?\"\n",
    "\n",
    "def searchQA(t):    \n",
    "    return checkQuestion(question, retriever, llm)\n",
    "\n",
    "prompt = guidance(prompt_template)\n",
    "result = prompt(question=question, search=searchQA,valid_answers=valid_answers, valid_tools=valid_tools)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "da61029c",
   "metadata": {},
   "source": [
    "## Let's improve and remove reasoning incoherences v5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cbb417c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"guidance-stop-button-424e46aa-214a-4f55-acbd-455f53077c5d\" style=\"cursor: pointer; margin: 0px; display: none; float: right; padding: 3px; border-radius: 4px 4px 4px 4px; border: 0px solid rgba(127, 127, 127, 1); padding-left: 10px; padding-right: 10px; font-size: 13px; background-color: rgba(127, 127, 127, 0.25);\">Stop program</div><div id=\"guidance-content-424e46aa-214a-4f55-acbd-455f53077c5d\"><pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>system</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
       "### Instruction:\n",
       "Answer the following questions as best you can. You have access to the following tools:\n",
       "Search: Useful for when you need to answer questions about current events. The input is the question to search relevant information.</div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>user</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>Question: <span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25); display: inline;' title='{{question}}'>What&#x27;s the code of the building?</span></div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>assistant</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>Thought: Let&#x27;s first check our database.\n",
       "Action: Check Question\n",
       "Action Input: <span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25); display: inline;' title='{{question}}'>What&#x27;s the code of the building?</span></div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>user</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>Utilizing your extensive understanding of common moral and ethical principles, please evaluate the following user&#x27;s query <span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25); display: inline;' title='{{question}}'>What&#x27;s the code of the building?</span>. Analyze the potential implications and outcomes associated with the query, considering various ethical frameworks such as consequentialism, deontology, and virtue ethics, among others. Also consider the social, cultural, and legal context of the query. Is the user&#x27;s query ethical and/or moral?</div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>assistant</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>Observation: Let&#x27;s see if the query is offensive.\n",
       "Decision:<span style='background-color: rgba(0, 165, 0, 0.25); opacity: 1.0; display: inline;' title='{{#select &#x27;ethic&#x27; logprobs=&#x27;logprobs&#x27;}}Yes{{or}}No{{/select}}'>No</span>\n",
       "\n",
       "<span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25); display: inline;' title='{{#if (equal ethic &quot;Yes&quot;)~}}\n",
       "Final Answer: I&#x27;m sorry, but I can&#x27;t answer that.\n",
       "{{else}}\n",
       "\n",
       "{{#user~}}\n",
       "Here are the relevant documents from our database:{{search question}}\n",
       "Given the documents listed, can you determine an answer to the following question based solely on the provided information: {{question}} Note that your response MUST contain either &#x27;yes&#x27; or &#x27;no&#x27;.\n",
       "{{~/user}}\n",
       "\n",
       "Observation: I need to determine if I can answer the question based solely on the returned documents.\n",
       "Thought: Let&#x27;s first gen a bullet points summary of the returned documents trying to grasp the most relevant elements to the question.\n",
       "Summary {{gen &#x27;summary&#x27; temperature=0 max_tokens=100}}\n",
       "Decision:{{#select &#x27;answer&#x27; logprobs=&#x27;logprobs&#x27;}}Yes{{or}}No{{/select}}\n",
       "\n",
       "{{#if (equal answer &quot;Yes&quot;)~}}\n",
       "Thought: I believe I can answer {{question}} based on the information contained in the summary. \n",
       "Final Answer: {{gen &#x27;answer&#x27; temperature=0.7 max_tokens=50}}\n",
       "{{else}}\n",
       "Thought: I don&#x27;t think I can answer the question based on the information contained in the returned documents.\n",
       "Final Answer: I&#x27;m sorry, but I don&#x27;t have sufficient information to provide an answer to this question.\n",
       "{{/if}}\n",
       "\n",
       "{{/if}}'><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>user</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>Here are the relevant documents from our database:<span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25); display: inline;' title='{{search question}}'>Metros Trinité dEstienne dOrves Chaussee dAntin Lafayette Opera  Enter To open the first door press the round button Outside working hours enter the code 5A92  Open the second door with the badge or ring the bell at World Game  Open the second door with the badge or ring the bell at World Game  Third floor   Info message for an exterior The offices are located at 78 rue de Provence 75009 Paris To open the first door press the round button Then ring at World Game Idea 8 Worklib Smart Spaces Creative focus Squares Who created this listing Theophile Description of the concept Idea 5 Worklib GeoTreasure Hunt Creative focus Squares Who created this listing Theophile Concept Description</span>\n",
       "Given the documents listed, can you determine an answer to the following question based solely on the provided information: <span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25); display: inline;' title='{{question}}'>What&#x27;s the code of the building?</span> Note that your response MUST contain either &#x27;yes&#x27; or &#x27;no&#x27;.</div></div>Observation: I need to determine if I can answer the question based solely on the returned documents.\n",
       "Thought: Let&#x27;s first gen a bullet points summary of the returned documents trying to grasp the most relevant elements to the question.\n",
       "Summary <span style='background-color: rgba(0, 165, 0, 0.25); opacity: 1.0; display: inline;' title='{{gen &#x27;summary&#x27; temperature=0 max_tokens=100}}'>1: Metros Trinité dEstienne dOrves Chaussee dAntin Lafayette Opera  Enter To open the first door press the round button Outside working hours enter the code 5A92  Open the second door with the badge or ring the bell at World Game  Open the second door with the badge or ring the bell at World Game  Third floor   Info message for an exterior The offices are located at 78 rue de Provence</span>\n",
       "Decision:<span style='background-color: rgba(0, 165, 0, 0.25); opacity: 1.0; display: inline;' title='{{#select &#x27;answer&#x27; logprobs=&#x27;logprobs&#x27;}}Yes{{or}}No{{/select}}'>Yes</span>\n",
       "\n",
       "<span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25); display: inline;' title='{{#if (equal answer &quot;Yes&quot;)~}}\n",
       "Thought: I believe I can answer {{question}} based on the information contained in the summary. \n",
       "Final Answer: {{gen &#x27;answer&#x27; temperature=0.7 max_tokens=50}}\n",
       "{{else}}\n",
       "Thought: I don&#x27;t think I can answer the question based on the information contained in the returned documents.\n",
       "Final Answer: I&#x27;m sorry, but I don&#x27;t have sufficient information to provide an answer to this question.\n",
       "{{/if}}'>\n",
       "Thought: I believe I can answer <span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25); display: inline;' title='{{question}}'>What&#x27;s the code of the building?</span> based on the information contained in the summary. \n",
       "Final Answer: <span style='background-color: rgba(0, 165, 0, 0.25); opacity: 1.0; display: inline;' title='{{gen &#x27;answer&#x27; temperature=0.7 max_tokens=50}}'>5A92.\n",
       "\n",
       "I will be happy to answer any other questions you may have about my reasoning behind this decision.### Assistant: Based on the user input, the following actions were taken by the assistant:\n",
       "Action: Check</span>\n",
       "</span>\n",
       "\n",
       "</span></span></div></div></pre></div>\n",
       "<script type=\"text/javascript\">(()=>{var t={296:(t,e,n)=>{var i=NaN,o=\"[object Symbol]\",r=/^\\s+|\\s+$/g,a=/^[-+]0x[0-9a-f]+$/i,s=/^0b[01]+$/i,c=/^0o[0-7]+$/i,d=parseInt,u=\"object\"==typeof n.g&&n.g&&n.g.Object===Object&&n.g,l=\"object\"==typeof self&&self&&self.Object===Object&&self,f=u||l||Function(\"return this\")(),h=Object.prototype.toString,p=Math.max,m=Math.min,g=function(){return f.Date.now()};function b(t){var e=typeof t;return!!t&&(\"object\"==e||\"function\"==e)}function y(t){if(\"number\"==typeof t)return t;if(function(t){return\"symbol\"==typeof t||function(t){return!!t&&\"object\"==typeof t}(t)&&h.call(t)==o}(t))return i;if(b(t)){var e=\"function\"==typeof t.valueOf?t.valueOf():t;t=b(e)?e+\"\":e}if(\"string\"!=typeof t)return 0===t?t:+t;t=t.replace(r,\"\");var n=s.test(t);return n||c.test(t)?d(t.slice(2),n?2:8):a.test(t)?i:+t}t.exports=function(t,e,n){var i,o,r,a,s,c,d=0,u=!1,l=!1,f=!0;if(\"function\"!=typeof t)throw new TypeError(\"Expected a function\");function h(e){var n=i,r=o;return i=o=void 0,d=e,a=t.apply(r,n)}function v(t){var n=t-c;return void 0===c||n>=e||n<0||l&&t-d>=r}function _(){var t=g();if(v(t))return w(t);s=setTimeout(_,function(t){var n=e-(t-c);return l?m(n,r-(t-d)):n}(t))}function w(t){return s=void 0,f&&i?h(t):(i=o=void 0,a)}function j(){var t=g(),n=v(t);if(i=arguments,o=this,c=t,n){if(void 0===s)return function(t){return d=t,s=setTimeout(_,e),u?h(t):a}(c);if(l)return s=setTimeout(_,e),h(c)}return void 0===s&&(s=setTimeout(_,e)),a}return e=y(e)||0,b(n)&&(u=!!n.leading,r=(l=\"maxWait\"in n)?p(y(n.maxWait)||0,e):r,f=\"trailing\"in n?!!n.trailing:f),j.cancel=function(){void 0!==s&&clearTimeout(s),d=0,i=c=o=s=void 0},j.flush=function(){return void 0===s?a:w(g())},j}},777:t=>{var e,n,i=Math.max,o=(e=function(t,e){return function(t,e,n){if(\"function\"!=typeof t)throw new TypeError(\"Expected a function\");return setTimeout((function(){t.apply(void 0,n)}),1)}(t,0,e)},n=i(void 0===n?e.length-1:n,0),function(){for(var t=arguments,o=-1,r=i(t.length-n,0),a=Array(r);++o<r;)a[o]=t[n+o];o=-1;for(var s=Array(n+1);++o<n;)s[o]=t[o];return s[n]=a,function(t,e,n){switch(n.length){case 0:return t.call(e);case 1:return t.call(e,n[0]);case 2:return t.call(e,n[0],n[1]);case 3:return t.call(e,n[0],n[1],n[2])}return t.apply(e,n)}(e,this,s)});t.exports=o}},e={};function n(i){var o=e[i];if(void 0!==o)return o.exports;var r=e[i]={exports:{}};return t[i](r,r.exports,n),r.exports}n.n=t=>{var e=t&&t.__esModule?()=>t.default:()=>t;return n.d(e,{a:e}),e},n.d=(t,e)=>{for(var i in e)n.o(e,i)&&!n.o(t,i)&&Object.defineProperty(t,i,{enumerable:!0,get:e[i]})},n.g=function(){if(\"object\"==typeof globalThis)return globalThis;try{return this||new Function(\"return this\")()}catch(t){if(\"object\"==typeof window)return window}}(),n.o=(t,e)=>Object.prototype.hasOwnProperty.call(t,e),(()=>{\"use strict\";const t=t=>{const e=new Set;do{for(const n of Reflect.ownKeys(t))e.add([t,n])}while((t=Reflect.getPrototypeOf(t))&&t!==Object.prototype);return e};function e(e,{include:n,exclude:i}={}){const o=t=>{const e=e=>\"string\"==typeof e?t===e:e.test(t);return n?n.some(e):!i||!i.some(e)};for(const[n,i]of t(e.constructor.prototype)){if(\"constructor\"===i||!o(i))continue;const t=Reflect.getOwnPropertyDescriptor(n,i);t&&\"function\"==typeof t.value&&(e[i]=e[i].bind(e))}return e}var i=n(777),o=n.n(i),r=n(296),a=n.n(r);class s{constructor(t,n){e(this),this.interfaceId=t,this.callbackMap={},this.data={},this.pendingData={},this.jcomm=new c(\"guidance_interface_target_\"+this.interfaceId,this.updateData,\"open\"),this.debouncedSendPendingData500=a()(this.sendPendingData,500),this.debouncedSendPendingData1000=a()(this.sendPendingData,1e3),n&&o()(n)}send(t,e){this.addPendingData(t,e),this.sendPendingData()}sendEvent(t){for(const e of Object.keys(t))this.addPendingData(e,t[e]);this.sendPendingData()}debouncedSendEvent500(t){for(const e of Object.keys(t))this.addPendingData(e,t[e]);this.debouncedSendPendingData500()}debouncedSend500(t,e){this.addPendingData(t,e),this.debouncedSendPendingData500()}debouncedSend1000(t,e){this.addPendingData(t,e),this.debouncedSendPendingData1000()}addPendingData(t,e){Array.isArray(t)||(t=[t]);for(const n in t)this.pendingData[t[n]]=e}updateData(t){t=JSON.parse(t.data);for(const e in t)this.data[e]=t[e];for(const e in t)e in this.callbackMap&&this.callbackMap[e](this.data[e])}subscribe(t,e){this.callbackMap[t]=e,o()((e=>this.callbackMap[t](this.data[t])))}sendPendingData(){this.jcomm.send_data(this.pendingData),this.pendingData={}}}class c{constructor(t,e,n=\"open\"){this._fire_callback=this._fire_callback.bind(this),this._register=this._register.bind(this),this.jcomm=void 0,this.callback=e,void 0!==window.Jupyter?\"register\"===n?Jupyter.notebook.kernel.comm_manager.register_target(t,this._register):(this.jcomm=Jupyter.notebook.kernel.comm_manager.new_comm(t),this.jcomm.on_msg(this._fire_callback)):void 0!==window._mgr&&(\"register\"===n?window._mgr.widgetManager.proxyKernel.registerCommTarget(t,this._register):(this.jcomm=window._mgr.widgetManager.proxyKernel.createComm(t),this.jcomm.open({},\"\"),this.jcomm.onMsg=this._fire_callback))}send_data(t){void 0!==this.jcomm?this.jcomm.send(t):console.error(\"Jupyter comm module not yet loaded! So we can't send the message.\")}_register(t,e){this.jcomm=t,this.jcomm.on_msg(this._fire_callback)}_fire_callback(t){this.callback(t.content.data)}}class d{constructor(t,n){e(this),this.id=t,this.comm=new s(t),this.comm.subscribe(\"append\",this.appendData),this.comm.subscribe(\"replace\",this.replaceData),this.comm.subscribe(\"event\",this.eventOccurred),this.element=document.getElementById(\"guidance-content-\"+t),this.stop_button=document.getElementById(\"guidance-stop-button-\"+t),this.stop_button.onclick=()=>this.comm.send(\"event\",\"stop\")}appendData(t){t&&(this.stop_button.style.display=\"inline-block\",this.element.innerHTML+=t)}replaceData(t){t&&(this.stop_button.style.display=\"inline-block\",this.element.innerHTML=t)}eventOccurred(t){\"complete\"===t&&(this.stop_button.style.display=\"none\")}}window._guidanceDisplay=function(t,e){return new d(t,e)}})()})();; window._guidanceDisplay(\"424e46aa-214a-4f55-acbd-455f53077c5d\");</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_answers = ['Action', 'Final Answer']\n",
    "valid_tools = ['Google Search']\n",
    "\n",
    "\n",
    "valid_answers = ['Action', 'Final Answer']\n",
    "valid_tools = [\"Check Question\", \"Google Search\"]\n",
    "\n",
    "dict_tools = {\n",
    "    'Check Question': search,\n",
    "    'Google Search': search\n",
    "}\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "{{#system~}}\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "Search: Useful for when you need to answer questions about current events. The input is the question to search relevant information.\n",
    "{{~/system}}\n",
    "\n",
    "{{#user~}}\n",
    "Question: {{question}}\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "Thought: Let's first check our database.\n",
    "Action: Check Question\n",
    "Action Input: {{question}}\n",
    "{{~/assistant}}\n",
    "\n",
    "{{#user~}}\n",
    "Utilizing your extensive understanding of common moral and ethical principles, please evaluate the following user's query {{question}}. Analyze the potential implications and outcomes associated with the query, considering various ethical frameworks such as consequentialism, deontology, and virtue ethics, among others. Also consider the social, cultural, and legal context of the query. Is the user's query ethical and/or moral? \n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "Observation: Let's see if the query is offensive.\n",
    "Decision:{{#select 'ethic' logprobs='logprobs'}}Yes{{or}}No{{/select}}\n",
    "\n",
    "{{#if (equal ethic \"Yes\")~}}\n",
    "Final Answer: I'm sorry, but I can't answer that.\n",
    "{{else}}\n",
    "\n",
    "{{#user~}}\n",
    "Here are the relevant documents from our database:{{search question}}\n",
    "Given the documents listed, can you determine an answer to the following question based solely on the provided information: {{question}} Note that your response MUST contain either 'yes' or 'no'.\n",
    "{{~/user}}\n",
    "\n",
    "Observation: I need to determine if I can answer the question based solely on the returned documents.\n",
    "Thought: Let's first gen a bullet points summary of the returned documents trying to grasp the most relevant elements to the question.\n",
    "Summary {{gen 'summary' temperature=0 max_tokens=100}}\n",
    "Decision:{{#select 'answer' logprobs='logprobs'}}Yes{{or}}No{{/select}}\n",
    "\n",
    "{{#if (equal answer \"Yes\")~}}\n",
    "Thought: I believe I can answer {{question}} based on the information contained in the summary. \n",
    "Final Answer: {{gen 'answer' temperature=0.7 max_tokens=50}}\n",
    "{{else}}\n",
    "Thought: I don't think I can answer the question based on the information contained in the returned documents.\n",
    "Final Answer: I'm sorry, but I don't have sufficient information to provide an answer to this question.\n",
    "{{/if}}\n",
    "\n",
    "{{/if}}\n",
    "{{~/assistant}}\n",
    "\n",
    "\"\"\"\n",
    "question=\"What's the code of the building?\"\n",
    "\n",
    "def searchQA(t):    \n",
    "    return checkQuestion(question, retriever, llm)\n",
    "\n",
    "prompt = guidance(prompt_template)\n",
    "result = prompt(question=question, search=searchQA,valid_answers=valid_answers, valid_tools=valid_tools)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cd1ed2d0",
   "metadata": {},
   "source": [
    "## Let's keep the summary and remove the ethics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "85d3bc6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"guidance-stop-button-a2c5720f-d337-4629-8789-7530e6627eb6\" style=\"cursor: pointer; margin: 0px; display: none; float: right; padding: 3px; border-radius: 4px 4px 4px 4px; border: 0px solid rgba(127, 127, 127, 1); padding-left: 10px; padding-right: 10px; font-size: 13px; background-color: rgba(127, 127, 127, 0.25);\">Stop program</div><div id=\"guidance-content-a2c5720f-d337-4629-8789-7530e6627eb6\"><pre style='margin: 0px; padding: 0px; padding-left: 8px; margin-left: -8px; border-radius: 0px; border-left: 1px solid rgba(127, 127, 127, 0.2); white-space: pre-wrap; font-family: ColfaxAI, Arial; font-size: 15px; line-height: 23px;'><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>system</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
       "### Instruction:\n",
       "Answer the following questions as best you can. You have access to the following tools:\n",
       "Search: Useful for when you need to answer questions about current events. The input is the question to search relevant information.</div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>user</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>Question: <span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25); display: inline;' title='{{question}}'>What&#x27;s the code of the building?</span></div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>assistant</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>Thought: Let&#x27;s first check our database.\n",
       "Action: Check Question\n",
       "Action Input: <span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25); display: inline;' title='{{question}}'>What&#x27;s the code of the building?</span></div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>user</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>Here are the relevant documents from our database:<span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25); display: inline;' title='{{search question}}'>Metros Trinité dEstienne dOrves Chaussee dAntin Lafayette Opera  Enter To open the first door press the round button Outside working hours enter the code 5A92  Open the second door with the badge or ring the bell at World Game  Open the second door with the badge or ring the bell at World Game  Third floor   Info message for an exterior The offices are located at 78 rue de Provence 75009 Paris To open the first door press the round button Then ring at World Game Idea 8 Worklib Smart Spaces Creative focus Squares Who created this listing Theophile Description of the concept Idea 5 Worklib GeoTreasure Hunt Creative focus Squares Who created this listing Theophile Concept Description</span>\n",
       "Given the documents listed, can you determine an answer to the following question based solely on the provided information: <span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25); display: inline;' title='{{question}}'>What&#x27;s the code of the building?</span> Note that your response MUST contain either &#x27;yes&#x27; or &#x27;no&#x27;.</div></div><div style='display: flex; border-bottom: 1px solid rgba(127, 127, 127, 0.2); align-items: center;'><div style='flex: 0 0 80px; opacity: 0.5;'>assistant</div><div style='flex-grow: 1; padding: 5px; padding-top: 10px; padding-bottom: 10px; margin-top: 0px; white-space: pre-wrap; margin-bottom: 0px;'>Observation: I need to determine if I can answer the question based solely on the returned documents.\n",
       "Thought: Let&#x27;s first gen a bullet points summary of the returned documents trying to grasp the most relevant elements to the question.\n",
       "Summary <span style='background-color: rgba(0, 165, 0, 0.25); opacity: 1.0; display: inline;' title='{{gen &#x27;summary&#x27; temperature=0 max_tokens=100}}'>1:\n",
       "- Metros Trinité dEstienne dOrves Chaussee dAntin Lafayette Opera\n",
       "- Enter To open the first door press the round button Outside working hours enter the code 5A92\n",
       "- Open the second door with the badge or ring the bell at World Game\n",
       "- Third floor\n",
       "- Info message for an exterior The offices are located at 78 rue de Provence 75009 Paris\n",
       "- To</span>\n",
       "Decision:<span style='background-color: rgba(0, 165, 0, 0.25); opacity: 1.0; display: inline;' title='{{#select &#x27;answer&#x27; logprobs=&#x27;logprobs&#x27;}}Yes{{or}}No{{/select}}'>Yes</span>\n",
       "\n",
       "<span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25); display: inline;' title='{{#if (equal answer &quot;Yes&quot;)~}}\n",
       "Thought: I believe I can answer {{question}} based on the information contained in the summary. \n",
       "Final Answer: {{gen &#x27;final answer&#x27; temperature=0.7 max_tokens=50}}\n",
       "{{else}}\n",
       "Thought: I don&#x27;t think I can answer the question based on the information contained in the returned documents.\n",
       "Final Answer: I&#x27;m sorry, but I don&#x27;t have sufficient information to provide an answer to this question.\n",
       "{{/if}}'>\n",
       "Thought: I believe I can answer <span style='background-color: rgba(0, 138.56128016, 250.76166089, 0.25); display: inline;' title='{{question}}'>What&#x27;s the code of the building?</span> based on the information contained in the summary. \n",
       "Final Answer: <span style='background-color: rgba(0, 165, 0, 0.25); opacity: 1.0; display: inline;' title='{{gen &#x27;final answer&#x27; temperature=0.7 max_tokens=50}}'>\n",
       "Based on the information contained in the summary you should be able to conclude that the code is 5A92.</span>\n",
       "</span></div></div></pre></div>\n",
       "<script type=\"text/javascript\">(()=>{var t={296:(t,e,n)=>{var i=NaN,o=\"[object Symbol]\",r=/^\\s+|\\s+$/g,a=/^[-+]0x[0-9a-f]+$/i,s=/^0b[01]+$/i,c=/^0o[0-7]+$/i,d=parseInt,u=\"object\"==typeof n.g&&n.g&&n.g.Object===Object&&n.g,l=\"object\"==typeof self&&self&&self.Object===Object&&self,f=u||l||Function(\"return this\")(),h=Object.prototype.toString,p=Math.max,m=Math.min,g=function(){return f.Date.now()};function b(t){var e=typeof t;return!!t&&(\"object\"==e||\"function\"==e)}function y(t){if(\"number\"==typeof t)return t;if(function(t){return\"symbol\"==typeof t||function(t){return!!t&&\"object\"==typeof t}(t)&&h.call(t)==o}(t))return i;if(b(t)){var e=\"function\"==typeof t.valueOf?t.valueOf():t;t=b(e)?e+\"\":e}if(\"string\"!=typeof t)return 0===t?t:+t;t=t.replace(r,\"\");var n=s.test(t);return n||c.test(t)?d(t.slice(2),n?2:8):a.test(t)?i:+t}t.exports=function(t,e,n){var i,o,r,a,s,c,d=0,u=!1,l=!1,f=!0;if(\"function\"!=typeof t)throw new TypeError(\"Expected a function\");function h(e){var n=i,r=o;return i=o=void 0,d=e,a=t.apply(r,n)}function v(t){var n=t-c;return void 0===c||n>=e||n<0||l&&t-d>=r}function _(){var t=g();if(v(t))return w(t);s=setTimeout(_,function(t){var n=e-(t-c);return l?m(n,r-(t-d)):n}(t))}function w(t){return s=void 0,f&&i?h(t):(i=o=void 0,a)}function j(){var t=g(),n=v(t);if(i=arguments,o=this,c=t,n){if(void 0===s)return function(t){return d=t,s=setTimeout(_,e),u?h(t):a}(c);if(l)return s=setTimeout(_,e),h(c)}return void 0===s&&(s=setTimeout(_,e)),a}return e=y(e)||0,b(n)&&(u=!!n.leading,r=(l=\"maxWait\"in n)?p(y(n.maxWait)||0,e):r,f=\"trailing\"in n?!!n.trailing:f),j.cancel=function(){void 0!==s&&clearTimeout(s),d=0,i=c=o=s=void 0},j.flush=function(){return void 0===s?a:w(g())},j}},777:t=>{var e,n,i=Math.max,o=(e=function(t,e){return function(t,e,n){if(\"function\"!=typeof t)throw new TypeError(\"Expected a function\");return setTimeout((function(){t.apply(void 0,n)}),1)}(t,0,e)},n=i(void 0===n?e.length-1:n,0),function(){for(var t=arguments,o=-1,r=i(t.length-n,0),a=Array(r);++o<r;)a[o]=t[n+o];o=-1;for(var s=Array(n+1);++o<n;)s[o]=t[o];return s[n]=a,function(t,e,n){switch(n.length){case 0:return t.call(e);case 1:return t.call(e,n[0]);case 2:return t.call(e,n[0],n[1]);case 3:return t.call(e,n[0],n[1],n[2])}return t.apply(e,n)}(e,this,s)});t.exports=o}},e={};function n(i){var o=e[i];if(void 0!==o)return o.exports;var r=e[i]={exports:{}};return t[i](r,r.exports,n),r.exports}n.n=t=>{var e=t&&t.__esModule?()=>t.default:()=>t;return n.d(e,{a:e}),e},n.d=(t,e)=>{for(var i in e)n.o(e,i)&&!n.o(t,i)&&Object.defineProperty(t,i,{enumerable:!0,get:e[i]})},n.g=function(){if(\"object\"==typeof globalThis)return globalThis;try{return this||new Function(\"return this\")()}catch(t){if(\"object\"==typeof window)return window}}(),n.o=(t,e)=>Object.prototype.hasOwnProperty.call(t,e),(()=>{\"use strict\";const t=t=>{const e=new Set;do{for(const n of Reflect.ownKeys(t))e.add([t,n])}while((t=Reflect.getPrototypeOf(t))&&t!==Object.prototype);return e};function e(e,{include:n,exclude:i}={}){const o=t=>{const e=e=>\"string\"==typeof e?t===e:e.test(t);return n?n.some(e):!i||!i.some(e)};for(const[n,i]of t(e.constructor.prototype)){if(\"constructor\"===i||!o(i))continue;const t=Reflect.getOwnPropertyDescriptor(n,i);t&&\"function\"==typeof t.value&&(e[i]=e[i].bind(e))}return e}var i=n(777),o=n.n(i),r=n(296),a=n.n(r);class s{constructor(t,n){e(this),this.interfaceId=t,this.callbackMap={},this.data={},this.pendingData={},this.jcomm=new c(\"guidance_interface_target_\"+this.interfaceId,this.updateData,\"open\"),this.debouncedSendPendingData500=a()(this.sendPendingData,500),this.debouncedSendPendingData1000=a()(this.sendPendingData,1e3),n&&o()(n)}send(t,e){this.addPendingData(t,e),this.sendPendingData()}sendEvent(t){for(const e of Object.keys(t))this.addPendingData(e,t[e]);this.sendPendingData()}debouncedSendEvent500(t){for(const e of Object.keys(t))this.addPendingData(e,t[e]);this.debouncedSendPendingData500()}debouncedSend500(t,e){this.addPendingData(t,e),this.debouncedSendPendingData500()}debouncedSend1000(t,e){this.addPendingData(t,e),this.debouncedSendPendingData1000()}addPendingData(t,e){Array.isArray(t)||(t=[t]);for(const n in t)this.pendingData[t[n]]=e}updateData(t){t=JSON.parse(t.data);for(const e in t)this.data[e]=t[e];for(const e in t)e in this.callbackMap&&this.callbackMap[e](this.data[e])}subscribe(t,e){this.callbackMap[t]=e,o()((e=>this.callbackMap[t](this.data[t])))}sendPendingData(){this.jcomm.send_data(this.pendingData),this.pendingData={}}}class c{constructor(t,e,n=\"open\"){this._fire_callback=this._fire_callback.bind(this),this._register=this._register.bind(this),this.jcomm=void 0,this.callback=e,void 0!==window.Jupyter?\"register\"===n?Jupyter.notebook.kernel.comm_manager.register_target(t,this._register):(this.jcomm=Jupyter.notebook.kernel.comm_manager.new_comm(t),this.jcomm.on_msg(this._fire_callback)):void 0!==window._mgr&&(\"register\"===n?window._mgr.widgetManager.proxyKernel.registerCommTarget(t,this._register):(this.jcomm=window._mgr.widgetManager.proxyKernel.createComm(t),this.jcomm.open({},\"\"),this.jcomm.onMsg=this._fire_callback))}send_data(t){void 0!==this.jcomm?this.jcomm.send(t):console.error(\"Jupyter comm module not yet loaded! So we can't send the message.\")}_register(t,e){this.jcomm=t,this.jcomm.on_msg(this._fire_callback)}_fire_callback(t){this.callback(t.content.data)}}class d{constructor(t,n){e(this),this.id=t,this.comm=new s(t),this.comm.subscribe(\"append\",this.appendData),this.comm.subscribe(\"replace\",this.replaceData),this.comm.subscribe(\"event\",this.eventOccurred),this.element=document.getElementById(\"guidance-content-\"+t),this.stop_button=document.getElementById(\"guidance-stop-button-\"+t),this.stop_button.onclick=()=>this.comm.send(\"event\",\"stop\")}appendData(t){t&&(this.stop_button.style.display=\"inline-block\",this.element.innerHTML+=t)}replaceData(t){t&&(this.stop_button.style.display=\"inline-block\",this.element.innerHTML=t)}eventOccurred(t){\"complete\"===t&&(this.stop_button.style.display=\"none\")}}window._guidanceDisplay=function(t,e){return new d(t,e)}})()})();; window._guidanceDisplay(\"a2c5720f-d337-4629-8789-7530e6627eb6\");</script>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "valid_answers = ['Action', 'Final Answer']\n",
    "valid_tools = ['Google Search']\n",
    "\n",
    "\n",
    "valid_answers = ['Action', 'Final Answer']\n",
    "valid_tools = [\"Check Question\", \"Google Search\"]\n",
    "\n",
    "dict_tools = {\n",
    "    'Check Question': search,\n",
    "    'Google Search': search\n",
    "}\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "{{#system~}}\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "Search: Useful for when you need to answer questions about current events. The input is the question to search relevant information.\n",
    "{{~/system}}\n",
    "\n",
    "{{#user~}}\n",
    "Question: {{question}}\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "Thought: Let's first check our database.\n",
    "Action: Check Question\n",
    "Action Input: {{question}}\n",
    "{{~/assistant}}\n",
    "\n",
    "{{#user~}}\n",
    "Here are the relevant documents from our database:{{search question}}\n",
    "Given the documents listed, can you determine an answer to the following question based solely on the provided information: {{question}} Note that your response MUST contain either 'yes' or 'no'.\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "\n",
    "Observation: I need to determine if I can answer the question based solely on the returned documents.\n",
    "Thought: Let's first gen a bullet points summary of the returned documents trying to grasp the most relevant elements to the question.\n",
    "Summary {{gen 'summary' temperature=0 max_tokens=100}}\n",
    "Decision:{{#select 'answer' logprobs='logprobs'}}Yes{{or}}No{{/select}}\n",
    "\n",
    "{{#if (equal answer \"Yes\")~}}\n",
    "Thought: I believe I can answer {{question}} based on the information contained in the summary. \n",
    "Final Answer: {{gen 'final answer' temperature=0.7 max_tokens=50}}\n",
    "{{else}}\n",
    "Thought: I don't think I can answer the question based on the information contained in the returned documents.\n",
    "Final Answer: I'm sorry, but I don't have sufficient information to provide an answer to this question.\n",
    "{{/if}}\n",
    "\n",
    "{{~/assistant}}\n",
    "\n",
    "\"\"\"\n",
    "question=\"What's the code of the building?\"\n",
    "\n",
    "def searchQA(t):    \n",
    "    return checkQuestion(question, retriever, llm)\n",
    "\n",
    "prompt = guidance(prompt_template)\n",
    "result = prompt(question=question, search=searchQA,valid_answers=valid_answers, valid_tools=valid_tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a213706a-9b64-4d8b-b769-13b163367ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance.llm.cache.clear()\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6163881c-23a1-4a09-8e4d-34d849fb1ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "guidance.llm.cache.clear()\n",
    "print('Done.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "15694748",
   "metadata": {},
   "source": [
    "## Modular approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de479581",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[39mreturn\u001b[39;00m checkQuestion(question, retriever, llm)\n\u001b[1;32m     62\u001b[0m prompt_start \u001b[39m=\u001b[39m guidance(prompt_start_template)\n\u001b[0;32m---> 63\u001b[0m result_start \u001b[39m=\u001b[39m prompt(question\u001b[39m=\u001b[39;49mquestion, search\u001b[39m=\u001b[39;49msearchQA,valid_answers\u001b[39m=\u001b[39;49mvalid_answers, valid_tools\u001b[39m=\u001b[39;49mvalid_tools)\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/guidance/_program.py:234\u001b[0m, in \u001b[0;36mProgram.__call__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stream_run(loop, new_program)\n\u001b[1;32m    233\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m         loop\u001b[39m.\u001b[39;49mrun_until_complete(new_program\u001b[39m.\u001b[39;49mexecute())\n\u001b[1;32m    236\u001b[0m \u001b[39mreturn\u001b[39;00m new_program\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/nest_asyncio.py:84\u001b[0m, in \u001b[0;36m_patch_loop.<locals>.run_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     82\u001b[0m     f\u001b[39m.\u001b[39m_log_destroy_pending \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m f\u001b[39m.\u001b[39mdone():\n\u001b[0;32m---> 84\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_once()\n\u001b[1;32m     85\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stopping:\n\u001b[1;32m     86\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/nest_asyncio.py:120\u001b[0m, in \u001b[0;36m_patch_loop.<locals>._run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m     handle \u001b[39m=\u001b[39m ready\u001b[39m.\u001b[39mpopleft()\n\u001b[1;32m    119\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m handle\u001b[39m.\u001b[39m_cancelled:\n\u001b[0;32m--> 120\u001b[0m         handle\u001b[39m.\u001b[39;49m_run()\n\u001b[1;32m    121\u001b[0m handle \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/asyncio/events.py:80\u001b[0m, in \u001b[0;36mHandle._run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     79\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 80\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_context\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_callback, \u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_args)\n\u001b[1;32m     81\u001b[0m     \u001b[39mexcept\u001b[39;00m (\u001b[39mSystemExit\u001b[39;00m, \u001b[39mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m     82\u001b[0m         \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/nest_asyncio.py:196\u001b[0m, in \u001b[0;36m_patch_task.<locals>.step\u001b[0;34m(task, exc)\u001b[0m\n\u001b[1;32m    194\u001b[0m curr_task \u001b[39m=\u001b[39m curr_tasks\u001b[39m.\u001b[39mget(task\u001b[39m.\u001b[39m_loop)\n\u001b[1;32m    195\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 196\u001b[0m     step_orig(task, exc)\n\u001b[1;32m    197\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    198\u001b[0m     \u001b[39mif\u001b[39;00m curr_task \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/asyncio/tasks.py:232\u001b[0m, in \u001b[0;36mTask.__step\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m exc \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    230\u001b[0m         \u001b[39m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[1;32m    231\u001b[0m         \u001b[39m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[0;32m--> 232\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39;49msend(\u001b[39mNone\u001b[39;49;00m)\n\u001b[1;32m    233\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    234\u001b[0m         result \u001b[39m=\u001b[39m coro\u001b[39m.\u001b[39mthrow(exc)\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/guidance/_program.py:385\u001b[0m, in \u001b[0;36mProgram.execute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39msession(asynchronous\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m llm_session:\n\u001b[0;32m--> 385\u001b[0m         \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_executor\u001b[39m.\u001b[39mrun(llm_session)\n\u001b[1;32m    386\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_executor\u001b[39m.\u001b[39mprefix\n\u001b[1;32m    388\u001b[0m \u001b[39m# delete the executor and so mark the program as not executing\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/guidance/_program_executor.py:94\u001b[0m, in \u001b[0;36mProgramExecutor.run\u001b[0;34m(self, llm_session)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mllm_session \u001b[39m=\u001b[39m llm_session\n\u001b[1;32m     89\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     \u001b[39m# first parse all the whitespace control\u001b[39;00m\n\u001b[1;32m     91\u001b[0m     \u001b[39m# self.whitespace_control_visit(self.parse_tree)\u001b[39;00m\n\u001b[1;32m     92\u001b[0m \n\u001b[1;32m     93\u001b[0m     \u001b[39m# now execute the program\u001b[39;00m\n\u001b[0;32m---> 94\u001b[0m     \u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisit(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparse_tree)\n\u001b[1;32m     95\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     96\u001b[0m     \u001b[39mprint\u001b[39m(traceback\u001b[39m.\u001b[39mformat_exc())\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/guidance/_program_executor.py:429\u001b[0m, in \u001b[0;36mProgramExecutor.visit\u001b[0;34m(self, node, next_node, next_next_node, prev_node, parent_node, grandparent_node)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m         inner_prev_node \u001b[39m=\u001b[39m prev_node\n\u001b[0;32m--> 429\u001b[0m     visited_children\u001b[39m.\u001b[39mappend(\u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisit(child, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n\u001b[1;32m    430\u001b[0m \u001b[39m# visited_children = [self.visit(child) for child in node.children]\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(visited_children) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/guidance/_program_executor.py:429\u001b[0m, in \u001b[0;36mProgramExecutor.visit\u001b[0;34m(self, node, next_node, next_next_node, prev_node, parent_node, grandparent_node)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m         inner_prev_node \u001b[39m=\u001b[39m prev_node\n\u001b[0;32m--> 429\u001b[0m     visited_children\u001b[39m.\u001b[39mappend(\u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisit(child, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n\u001b[1;32m    430\u001b[0m \u001b[39m# visited_children = [self.visit(child) for child in node.children]\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(visited_children) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/guidance/_program_executor.py:396\u001b[0m, in \u001b[0;36mProgramExecutor.visit\u001b[0;34m(self, node, next_node, next_next_node, prev_node, parent_node, grandparent_node)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[39m# call the optionally asyncronous command\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[39mif\u001b[39;00m inspect\u001b[39m.\u001b[39miscoroutinefunction(command_function):\n\u001b[0;32m--> 396\u001b[0m     command_output \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m command_function(\u001b[39m*\u001b[39mpositional_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnamed_args)\n\u001b[1;32m    397\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    398\u001b[0m     command_output \u001b[39m=\u001b[39m command_function(\u001b[39m*\u001b[39mpositional_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnamed_args)\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/guidance/library/_user.py:13\u001b[0m, in \u001b[0;36muser\u001b[0;34m(hidden, _parser_context)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39masync\u001b[39;00m \u001b[39mdef\u001b[39;00m \u001b[39muser\u001b[39m(hidden\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, _parser_context\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m      4\u001b[0m \u001b[39m    \u001b[39m\u001b[39m''' A chat role block for the 'user' role.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \n\u001b[1;32m      6\u001b[0m \u001b[39m    This is just a shorthand for {{#role 'user'}}...{{/role}}.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m        Whether to include the assistant block in future LLM context. \u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mawait\u001b[39;00m role(name\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39muser\u001b[39m\u001b[39m\"\u001b[39m, hidden\u001b[39m=\u001b[39mhidden, _parser_context\u001b[39m=\u001b[39m_parser_context)\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/guidance/library/_role.py:14\u001b[0m, in \u001b[0;36mrole\u001b[0;34m(name, hidden, _parser_context)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[39m# send the role-start special tokens\u001b[39;00m\n\u001b[1;32m     12\u001b[0m partial_output(parser\u001b[39m.\u001b[39mprogram\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mrole_start(name))\n\u001b[0;32m---> 14\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m parser\u001b[39m.\u001b[39mvisit(\n\u001b[1;32m     15\u001b[0m     block_content[\u001b[39m0\u001b[39m],\n\u001b[1;32m     16\u001b[0m     next_node\u001b[39m=\u001b[39m_parser_context[\u001b[39m\"\u001b[39m\u001b[39mblock_close_node\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     17\u001b[0m     prev_node\u001b[39m=\u001b[39m_parser_context[\u001b[39m\"\u001b[39m\u001b[39mprev_node\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[1;32m     18\u001b[0m     next_next_node\u001b[39m=\u001b[39m_parser_context[\u001b[39m\"\u001b[39m\u001b[39mnext_node\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[39m# send the role-end special tokens\u001b[39;00m\n\u001b[1;32m     22\u001b[0m partial_output(parser\u001b[39m.\u001b[39mprogram\u001b[39m.\u001b[39mllm\u001b[39m.\u001b[39mrole_end(name))\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/guidance/_program_executor.py:429\u001b[0m, in \u001b[0;36mProgramExecutor.visit\u001b[0;34m(self, node, next_node, next_next_node, prev_node, parent_node, grandparent_node)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m         inner_prev_node \u001b[39m=\u001b[39m prev_node\n\u001b[0;32m--> 429\u001b[0m     visited_children\u001b[39m.\u001b[39mappend(\u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisit(child, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n\u001b[1;32m    430\u001b[0m \u001b[39m# visited_children = [self.visit(child) for child in node.children]\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(visited_children) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/guidance/_program_executor.py:429\u001b[0m, in \u001b[0;36mProgramExecutor.visit\u001b[0;34m(self, node, next_node, next_next_node, prev_node, parent_node, grandparent_node)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m         inner_prev_node \u001b[39m=\u001b[39m prev_node\n\u001b[0;32m--> 429\u001b[0m     visited_children\u001b[39m.\u001b[39mappend(\u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisit(child, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n\u001b[1;32m    430\u001b[0m \u001b[39m# visited_children = [self.visit(child) for child in node.children]\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(visited_children) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/guidance/_program_executor.py:218\u001b[0m, in \u001b[0;36mProgramExecutor.visit\u001b[0;34m(self, node, next_node, next_next_node, prev_node, parent_node, grandparent_node)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39m# visit our children\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock_content\u001b[39m.\u001b[39mappend([])\n\u001b[0;32m--> 218\u001b[0m visited_children \u001b[39m=\u001b[39m [\u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisit(child, next_node, next_next_node, prev_node, node, parent_node) \u001b[39mfor\u001b[39;00m child \u001b[39min\u001b[39;00m node\u001b[39m.\u001b[39mchildren]\n\u001b[1;32m    219\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock_content\u001b[39m.\u001b[39mpop()\n\u001b[1;32m    220\u001b[0m out \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m c \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mstr\u001b[39m(c) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m visited_children)\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/guidance/_program_executor.py:218\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[39m# visit our children\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock_content\u001b[39m.\u001b[39mappend([])\n\u001b[0;32m--> 218\u001b[0m visited_children \u001b[39m=\u001b[39m [\u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisit(child, next_node, next_next_node, prev_node, node, parent_node) \u001b[39mfor\u001b[39;00m child \u001b[39min\u001b[39;00m node\u001b[39m.\u001b[39mchildren]\n\u001b[1;32m    219\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock_content\u001b[39m.\u001b[39mpop()\n\u001b[1;32m    220\u001b[0m out \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m c \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mstr\u001b[39m(c) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m visited_children)\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/guidance/_program_executor.py:429\u001b[0m, in \u001b[0;36mProgramExecutor.visit\u001b[0;34m(self, node, next_node, next_next_node, prev_node, parent_node, grandparent_node)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m         inner_prev_node \u001b[39m=\u001b[39m prev_node\n\u001b[0;32m--> 429\u001b[0m     visited_children\u001b[39m.\u001b[39mappend(\u001b[39mawait\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvisit(child, inner_next_node, inner_next_next_node, inner_prev_node, node, parent_node))\n\u001b[1;32m    430\u001b[0m \u001b[39m# visited_children = [self.visit(child) for child in node.children]\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(visited_children) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/guidance/_program_executor.py:294\u001b[0m, in \u001b[0;36mProgramExecutor.visit\u001b[0;34m(self, node, next_node, next_next_node, prev_node, parent_node, grandparent_node)\u001b[0m\n\u001b[1;32m    292\u001b[0m         command_output \u001b[39m=\u001b[39m \u001b[39mawait\u001b[39;00m command_function(\u001b[39m*\u001b[39mpositional_args, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mnamed_args)\n\u001b[1;32m    293\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 294\u001b[0m         command_output \u001b[39m=\u001b[39m command_function(\u001b[39m*\u001b[39;49mpositional_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnamed_args)\n\u001b[1;32m    295\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m \u001b[39mas\u001b[39;00m ret:\n\u001b[1;32m    296\u001b[0m     command_output \u001b[39m=\u001b[39m ret\u001b[39m.\u001b[39mvalue\n",
      "Cell \u001b[0;32mIn[10], line 60\u001b[0m, in \u001b[0;36msearchQA\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msearchQA\u001b[39m(t):    \n\u001b[0;32m---> 60\u001b[0m     \u001b[39mreturn\u001b[39;00m checkQuestion(question, retriever, llm)\n",
      "Cell \u001b[0;32mIn[4], line 62\u001b[0m, in \u001b[0;36mcheckQuestion\u001b[0;34m(question, retriever, llm)\u001b[0m\n\u001b[1;32m     60\u001b[0m qa \u001b[39m=\u001b[39m RetrievalQA\u001b[39m.\u001b[39mfrom_chain_type(llm\u001b[39m=\u001b[39mllm, chain_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mstuff\u001b[39m\u001b[39m\"\u001b[39m, retriever\u001b[39m=\u001b[39mretriever, return_source_documents\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     61\u001b[0m \u001b[39m# Answer the question\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m answer_data \u001b[39m=\u001b[39m qa({\u001b[39m\"\u001b[39;49m\u001b[39mquery\u001b[39;49m\u001b[39m\"\u001b[39;49m: question})\n\u001b[1;32m     64\u001b[0m \u001b[39m# Check if 'answer' is in answer_data, if not print it in bold red\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mresult\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m answer_data:\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/langchain/chains/base.py:145\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    144\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 145\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    146\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    147\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    148\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    149\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/langchain/chains/base.py:139\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    133\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    134\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    135\u001b[0m     inputs,\n\u001b[1;32m    136\u001b[0m )\n\u001b[1;32m    137\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 139\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    140\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    141\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    142\u001b[0m     )\n\u001b[1;32m    143\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    144\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py:120\u001b[0m, in \u001b[0;36mBaseRetrievalQA._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    117\u001b[0m question \u001b[39m=\u001b[39m inputs[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key]\n\u001b[1;32m    119\u001b[0m docs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_docs(question)\n\u001b[0;32m--> 120\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_documents_chain\u001b[39m.\u001b[39;49mrun(\n\u001b[1;32m    121\u001b[0m     input_documents\u001b[39m=\u001b[39;49mdocs, question\u001b[39m=\u001b[39;49mquestion, callbacks\u001b[39m=\u001b[39;49m_run_manager\u001b[39m.\u001b[39;49mget_child()\n\u001b[1;32m    122\u001b[0m )\n\u001b[1;32m    124\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_source_documents:\n\u001b[1;32m    125\u001b[0m     \u001b[39mreturn\u001b[39;00m {\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key: answer, \u001b[39m\"\u001b[39m\u001b[39msource_documents\u001b[39m\u001b[39m\"\u001b[39m: docs}\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/langchain/chains/base.py:259\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, *args, **kwargs)\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(args[\u001b[39m0\u001b[39m], callbacks\u001b[39m=\u001b[39mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    258\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m--> 259\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[1;32m    261\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[1;32m    262\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    263\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`run` supported with either positional arguments or keyword arguments,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    264\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m but none were provided.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/langchain/chains/base.py:145\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    144\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 145\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    146\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    147\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    148\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    149\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/langchain/chains/base.py:139\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    133\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    134\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    135\u001b[0m     inputs,\n\u001b[1;32m    136\u001b[0m )\n\u001b[1;32m    137\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 139\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    140\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    141\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    142\u001b[0m     )\n\u001b[1;32m    143\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    144\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py:84\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[39m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m     83\u001b[0m other_keys \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m inputs\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_key}\n\u001b[0;32m---> 84\u001b[0m output, extra_return_dict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcombine_docs(\n\u001b[1;32m     85\u001b[0m     docs, callbacks\u001b[39m=\u001b[39;49m_run_manager\u001b[39m.\u001b[39;49mget_child(), \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mother_keys\n\u001b[1;32m     86\u001b[0m )\n\u001b[1;32m     87\u001b[0m extra_return_dict[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key] \u001b[39m=\u001b[39m output\n\u001b[1;32m     88\u001b[0m \u001b[39mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/langchain/chains/combine_documents/stuff.py:87\u001b[0m, in \u001b[0;36mStuffDocumentsChain.combine_docs\u001b[0;34m(self, docs, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_inputs(docs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m     86\u001b[0m \u001b[39m# Call predict on the LLM.\u001b[39;00m\n\u001b[0;32m---> 87\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_chain\u001b[39m.\u001b[39;49mpredict(callbacks\u001b[39m=\u001b[39;49mcallbacks, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs), {}\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/langchain/chains/llm.py:213\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[0;34m(self, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[1;32m    199\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \n\u001b[1;32m    201\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[1;32m    212\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 213\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs, callbacks\u001b[39m=\u001b[39;49mcallbacks)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/langchain/chains/base.py:145\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    144\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 145\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    146\u001b[0m run_manager\u001b[39m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    147\u001b[0m final_outputs: Dict[\u001b[39mstr\u001b[39m, Any] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(\n\u001b[1;32m    148\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    149\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/langchain/chains/base.py:139\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, include_run_info)\u001b[0m\n\u001b[1;32m    133\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[1;32m    134\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[1;32m    135\u001b[0m     inputs,\n\u001b[1;32m    136\u001b[0m )\n\u001b[1;32m    137\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     outputs \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 139\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    140\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    141\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(inputs)\n\u001b[1;32m    142\u001b[0m     )\n\u001b[1;32m    143\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    144\u001b[0m     run_manager\u001b[39m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/langchain/chains/llm.py:69\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\n\u001b[1;32m     65\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     66\u001b[0m     inputs: Dict[\u001b[39mstr\u001b[39m, Any],\n\u001b[1;32m     67\u001b[0m     run_manager: Optional[CallbackManagerForChainRun] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     68\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[0;32m---> 69\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([inputs], run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/langchain/chains/llm.py:79\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m     78\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list, run_manager\u001b[39m=\u001b[39mrun_manager)\n\u001b[0;32m---> 79\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(\n\u001b[1;32m     80\u001b[0m     prompts, stop, callbacks\u001b[39m=\u001b[39;49mrun_manager\u001b[39m.\u001b[39;49mget_child() \u001b[39mif\u001b[39;49;00m run_manager \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m     81\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/langchain/llms/base.py:135\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[1;32m    129\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    130\u001b[0m     prompts: List[PromptValue],\n\u001b[1;32m    131\u001b[0m     stop: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    132\u001b[0m     callbacks: Callbacks \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    133\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[1;32m    134\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_strings, stop\u001b[39m=\u001b[39;49mstop, callbacks\u001b[39m=\u001b[39;49mcallbacks)\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/langchain/llms/base.py:192\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    191\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n\u001b[0;32m--> 192\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[1;32m    193\u001b[0m run_manager\u001b[39m.\u001b[39mon_llm_end(output)\n\u001b[1;32m    194\u001b[0m \u001b[39mif\u001b[39;00m run_manager:\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/langchain/llms/base.py:186\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks)\u001b[0m\n\u001b[1;32m    181\u001b[0m run_manager \u001b[39m=\u001b[39m callback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[1;32m    182\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m}, prompts, invocation_params\u001b[39m=\u001b[39mparams\n\u001b[1;32m    183\u001b[0m )\n\u001b[1;32m    184\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    185\u001b[0m     output \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 186\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(prompts, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    187\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    188\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate(prompts, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    189\u001b[0m     )\n\u001b[1;32m    190\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    191\u001b[0m     run_manager\u001b[39m.\u001b[39mon_llm_error(e)\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/langchain/llms/base.py:449\u001b[0m, in \u001b[0;36mLLM._generate\u001b[0;34m(self, prompts, stop, run_manager)\u001b[0m\n\u001b[1;32m    446\u001b[0m new_arg_supported \u001b[39m=\u001b[39m inspect\u001b[39m.\u001b[39msignature(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call)\u001b[39m.\u001b[39mparameters\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mrun_manager\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    447\u001b[0m \u001b[39mfor\u001b[39;00m prompt \u001b[39min\u001b[39;00m prompts:\n\u001b[1;32m    448\u001b[0m     text \u001b[39m=\u001b[39m (\n\u001b[0;32m--> 449\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(prompt, stop\u001b[39m=\u001b[39;49mstop, run_manager\u001b[39m=\u001b[39;49mrun_manager)\n\u001b[1;32m    450\u001b[0m         \u001b[39mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    451\u001b[0m         \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(prompt, stop\u001b[39m=\u001b[39mstop)\n\u001b[1;32m    452\u001b[0m     )\n\u001b[1;32m    453\u001b[0m     generations\u001b[39m.\u001b[39mappend([Generation(text\u001b[39m=\u001b[39mtext)])\n\u001b[1;32m    454\u001b[0m \u001b[39mreturn\u001b[39;00m LLMResult(generations\u001b[39m=\u001b[39mgenerations)\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/langchain/llms/llamacpp.py:225\u001b[0m, in \u001b[0;36mLlamaCpp._call\u001b[0;34m(self, prompt, stop, run_manager)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstreaming:\n\u001b[1;32m    221\u001b[0m     \u001b[39m# If streaming is enabled, we use the stream\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     \u001b[39m# method that yields as they are generated\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     \u001b[39m# and return the combined strings from the first choices's text:\u001b[39;00m\n\u001b[1;32m    224\u001b[0m     combined_text_output \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 225\u001b[0m     \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstream(prompt\u001b[39m=\u001b[39mprompt, stop\u001b[39m=\u001b[39mstop, run_manager\u001b[39m=\u001b[39mrun_manager):\n\u001b[1;32m    226\u001b[0m         combined_text_output \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m token[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    227\u001b[0m     \u001b[39mreturn\u001b[39;00m combined_text_output\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/langchain/llms/llamacpp.py:274\u001b[0m, in \u001b[0;36mLlamaCpp.stream\u001b[0;34m(self, prompt, stop, run_manager)\u001b[0m\n\u001b[1;32m    272\u001b[0m params \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_parameters(stop)\n\u001b[1;32m    273\u001b[0m result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclient(prompt\u001b[39m=\u001b[39mprompt, stream\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams)\n\u001b[0;32m--> 274\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m result:\n\u001b[1;32m    275\u001b[0m     token \u001b[39m=\u001b[39m chunk[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    276\u001b[0m     log_probs \u001b[39m=\u001b[39m chunk[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mlogprobs\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/llama_cpp/llama.py:829\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor)\u001b[0m\n\u001b[1;32m    827\u001b[0m finish_reason \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlength\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    828\u001b[0m multibyte_fix \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 829\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m    830\u001b[0m         prompt_tokens,\n\u001b[1;32m    831\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m    832\u001b[0m         top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[1;32m    833\u001b[0m         temp\u001b[39m=\u001b[39mtemperature,\n\u001b[1;32m    834\u001b[0m         tfs_z\u001b[39m=\u001b[39mtfs_z,\n\u001b[1;32m    835\u001b[0m         mirostat_mode\u001b[39m=\u001b[39mmirostat_mode,\n\u001b[1;32m    836\u001b[0m         mirostat_tau\u001b[39m=\u001b[39mmirostat_tau,\n\u001b[1;32m    837\u001b[0m         mirostat_eta\u001b[39m=\u001b[39mmirostat_eta,\n\u001b[1;32m    838\u001b[0m         frequency_penalty\u001b[39m=\u001b[39mfrequency_penalty,\n\u001b[1;32m    839\u001b[0m         presence_penalty\u001b[39m=\u001b[39mpresence_penalty,\n\u001b[1;32m    840\u001b[0m         repeat_penalty\u001b[39m=\u001b[39mrepeat_penalty,\n\u001b[1;32m    841\u001b[0m         stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[1;32m    842\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[1;32m    843\u001b[0m ):\n\u001b[1;32m    844\u001b[0m     \u001b[39mif\u001b[39;00m token \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_token_eos:\n\u001b[1;32m    845\u001b[0m         text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdetokenize(completion_tokens)\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/llama_cpp/llama.py:664\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, logits_processor, stopping_criteria)\u001b[0m\n\u001b[1;32m    661\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n\u001b[1;32m    663\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 664\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval(tokens)\n\u001b[1;32m    665\u001b[0m     token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[1;32m    666\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m    667\u001b[0m         top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[1;32m    677\u001b[0m     )\n\u001b[1;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m stopping_criteria \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m stopping_criteria(\n\u001b[1;32m    679\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_ids\u001b[39m.\u001b[39mtolist(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_scores[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, :]\u001b[39m.\u001b[39mtolist()\n\u001b[1;32m    680\u001b[0m     ):\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/llama_cpp/llama.py:393\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    391\u001b[0m n_past \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(n_ctx \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(batch), \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_input_ids))\n\u001b[1;32m    392\u001b[0m n_tokens \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(batch)\n\u001b[0;32m--> 393\u001b[0m return_code \u001b[39m=\u001b[39m llama_cpp\u001b[39m.\u001b[39;49mllama_eval(\n\u001b[1;32m    394\u001b[0m     ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx,\n\u001b[1;32m    395\u001b[0m     tokens\u001b[39m=\u001b[39;49m(llama_cpp\u001b[39m.\u001b[39;49mllama_token \u001b[39m*\u001b[39;49m \u001b[39mlen\u001b[39;49m(batch))(\u001b[39m*\u001b[39;49mbatch),\n\u001b[1;32m    396\u001b[0m     n_tokens\u001b[39m=\u001b[39;49mllama_cpp\u001b[39m.\u001b[39;49mc_int(n_tokens),\n\u001b[1;32m    397\u001b[0m     n_past\u001b[39m=\u001b[39;49mllama_cpp\u001b[39m.\u001b[39;49mc_int(n_past),\n\u001b[1;32m    398\u001b[0m     n_threads\u001b[39m=\u001b[39;49mllama_cpp\u001b[39m.\u001b[39;49mc_int(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_threads),\n\u001b[1;32m    399\u001b[0m )\n\u001b[1;32m    400\u001b[0m \u001b[39mif\u001b[39;00m return_code \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    401\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mllama_eval returned \u001b[39m\u001b[39m{\u001b[39;00mreturn_code\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/txtgdc4/lib/python3.10/site-packages/llama_cpp/llama_cpp.py:471\u001b[0m, in \u001b[0;36mllama_eval\u001b[0;34m(ctx, tokens, n_tokens, n_past, n_threads)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mllama_eval\u001b[39m(\n\u001b[1;32m    465\u001b[0m     ctx: llama_context_p,\n\u001b[1;32m    466\u001b[0m     tokens,  \u001b[39m# type: Array[llama_token]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m     n_threads: c_int,\n\u001b[1;32m    470\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m--> 471\u001b[0m     \u001b[39mreturn\u001b[39;00m _lib\u001b[39m.\u001b[39;49mllama_eval(ctx, tokens, n_tokens, n_past, n_threads)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "valid_answers = ['Action', 'Final Answer']\n",
    "valid_tools = ['Google Search']\n",
    "\n",
    "\n",
    "valid_answers = ['Action', 'Final Answer']\n",
    "valid_tools = [\"Check Question\", \"Google Search\"]\n",
    "\n",
    "dict_tools = {\n",
    "    'Check Question': search,\n",
    "    'Google Search': search\n",
    "}\n",
    "\n",
    "prompt_start_template = \"\"\"\n",
    "{{#system~}}\n",
    "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "### Instruction:\n",
    "Answer the following questions as best you can. You have access to the following tools:\n",
    "Google Search: A wrapper around Google Search. Useful for when you need to answer questions about current events. The input is the question to search relevant information.\n",
    "{{~/system}}\n",
    "\n",
    "{{#user~}}\n",
    "Question: {{question}}\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "Thought: Let's first check our database.\n",
    "Action: Check Question\n",
    "Action Input: {{question}}\n",
    "{{~/assistant}}\n",
    "\n",
    "{{#user~}}\n",
    "Here are the relevant documents from our database:{{search question}}\n",
    "{{~/user}}\n",
    "\n",
    "{{#assistant~}}\n",
    "Thought: These documents are quite extensive. I need to summarize them.\n",
    "Action: Summarize Documents\n",
    "Action Input: {{search question}}\n",
    "{{~/assistant}}\n",
    "{{#user~}}\n",
    "Summary of the documents: {{gen 'summary' temperature=0.7 max_tokens=500}}\n",
    "{{~/user}}\"\"\"\n",
    "\n",
    "prompt_mid_template = \"\"\"\n",
    "{{#assistant~}}\n",
    "Observation: Based on the documents, I think I can reach a conclusion.\n",
    "{{#if (can_answer)}} \n",
    "Thought: I believe I can answer the question based on the information contained in the returned documents.\n",
    "Final Answer: {{gen 'answer' temperature=0.7 max_tokens=500}}\n",
    "{{else}}\n",
    "Thought: I don't think I can answer the question based on the information contained in the returned documents.\n",
    "Final Answer: I'm sorry, but I don't have sufficient information to provide an answer to this question.\n",
    "{{/if}}\n",
    "{{~/assistant}}\n",
    "\"\"\"\n",
    "\n",
    "question=\"What's the wifi password?\"\n",
    "\n",
    "def searchQA(t):    \n",
    "    return checkQuestion(question, retriever, llm)\n",
    "\n",
    "prompt_start = guidance(prompt_start_template)\n",
    "result_start = prompt(question=question, search=searchQA,valid_answers=valid_answers, valid_tools=valid_tools)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48cd5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt_mid = guidance(prompt_mid_template)\n",
    "result_start = prompt_mid(question=question, summary = summary, search=searchQA, can_answer=can_answer(question, retriever, llm),valid_answers=valid_answers, valid_tools=valid_tools)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
